{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568f5f07",
   "metadata": {},
   "source": [
    "# Project 2: Customer‑Support Chatbot for an E-Commerce Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe2a04",
   "metadata": {},
   "source": [
    "Welcome! In this project, you'll build a **chatbot** that answers customer service questions about Everstorm Outfitters, an imaginary e-commerce store.\n",
    "\n",
    "Run each cell in order. Feel free to modify them as you go to better understand each tool and search the web or look online for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e17b2",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* **Ingest & chunk** unstructured docs  \n",
    "* **Embed** chunks and **index** with *FAISS*  \n",
    "* **Retrieve** context and **craft prompts**  \n",
    "* **Run** an open‑weight LLM locally with *Ollama*  \n",
    "* **Build** a Retrieval-Augmented Generation (RAG) chain\n",
    "* **Package** the chat loop in a minimal **Streamlit** web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46b9cc",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "We will build a RAG-based chatbot in **six** steps:\n",
    "\n",
    "1. **Environment setup**\n",
    "2. **Data preparation**  \n",
    "   a. Load source documents  \n",
    "   b. Chunk the text  \n",
    "3. **Build a retriever**  \n",
    "   a. Generate embeddings  \n",
    "   b. Build a FAISS vector index  \n",
    "4. **Build a generation engine**. Load the *Gemma3-1B* model through Ollama and run a sanity check.  \n",
    "5. **Build a RAG**. Connect the system prompt, retriever, and LLM together. \n",
    "6. **(Optional) Streamlit UI**. Wrap everything in a simple web app so users can chat with the bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a01819c",
   "metadata": {},
   "source": [
    "## 1 - Environment setup\n",
    "\n",
    "We use conda to manage our project dependencies and ensure everyone has a consistent setup. Conda is an open-source package and environment manager that makes it easy to install libraries and switch between isolated environments. To learn more about conda, you can read: https://docs.conda.io/en/latest/\n",
    "\n",
    "Create and activate a clean *conda* environment and install the required packages. If you don't have conda installed, visit https://www.anaconda.com/docs/getting-started/miniconda/main.\n",
    "\n",
    "\n",
    "Open your terminal, navigate to the project folder where this notebook is located, and run the following commands.\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml && conda activate rag-chatbot\n",
    "\n",
    "# (Optional but recommended) Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=rag-chatbot --display-name \"rag-chatbot\"\n",
    "```\n",
    "Once this is done, you can select “rag-chatbot” from the Kernel → Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "\n",
    "> Behind the scenes:\n",
    "> * Conda reads `environment.yml`, solves all pinned dependencies, and builds an isolated environment named `rag-chatbot`.\n",
    "> * When it reaches the file’s \"pip:\" section, Conda automatically invokes pip to install any remaining Python-only packages so the whole stack be available for the project.\n",
    "> * Registering the kernel makes your new environment visible to Jupyter, so the notebook runs inside the same environment you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a29e43",
   "metadata": {},
   "source": [
    "Let's import required libraries and print a message if we're not **missing packages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b49fcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate text embeddings using OpenAI or Hugging Face models\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1. OpenAI Embeddings are in the dedicated openai package\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 2. HuggingFace and SentenceTransformer Embeddings are in the community package\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "# Import standard libraries for file handling and text processing\n",
    "import os, pathlib, textwrap, glob\n",
    "\n",
    "# Load documents from various sources (URLs, text files, PDFs)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Split long texts into smaller, manageable chunks for embedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store to store and retrieve embeddings efficiently using FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Generate text embeddings using OpenAI or Hugging Face models\n",
    "# 1. OpenAI Embeddings are in the dedicated openai package\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "# 2. HuggingFace and SentenceTransformer Embeddings are in the community package\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "# Use local LLMs (e.g., via Ollama) for response generation\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Build a retrieval chain that combines a retriever, a prompt, and an LLM\n",
    "from langchain_community.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create prompts for the RAG system\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"✅ Libraries imported! You're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaacfdc",
   "metadata": {},
   "source": [
    "## 2 - Data preparation\n",
    "The goal of this step is to turn all reference documents into small chunks of text that a retriever can index and search. These documents typically come from:\n",
    "* PDF files: local documents such as policies, user manuals, or guides.\n",
    "* Web pages (HTML): online documentation, blog posts, or help articles.\n",
    "\n",
    "In this step, we perform two actions:\n",
    "* **Ingesting**: load every PDF and collect the raw text in a list named `raw_docs`.\n",
    "* **Chunking**: split each document into small, overlapping chunks so later steps can match a user query to the most relevant passage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150689",
   "metadata": {},
   "source": [
    "### 2.1 - Ingest source documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b461a",
   "metadata": {},
   "source": [
    "We can use different libraries to load and process PDFs. A quick web search will show several options, each with its own strengths. In this case, we’ll use PyPDFLoader from LangChain, which makes it easy to extract text from PDF files for downstream processing. To learn more about how to use it, refer to: https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/\n",
    "\n",
    "Use **PyPDFLoader** to load every PDF whose filename matches `data/Everstorm_*.pdf` and collect all pages in a list called `raw_docs`. The content of these PDFs is synthetically generated for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff055a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 PDF pages from 4 files.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Return_and_exchange_policy', 'source': 'data/Everstorm_Return_and_exchange_policy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Everstorm  Outfitters    RETURN  &  EXCHANGE  POLICY    Document  ROX-2025-05   Easy-Fit  Promise    If  your  gear  doesn’t  fit  or  just  isn’t  your  vibe,  send  it  back  within  **30  days**  of  delivery  for  a  refund  or  free  size  exchange.   Eligibility  Checklist    ●  Unworn,  unwashed,  no  odors,  tags  attached    ●  Original  shoe  box  (footwear)  placed  inside  outer  carton    ●  Electronics  (power-banks,  headlamps)  unopened  unless  faulty   How  to  Start    ●  Visit  everstorm.example/returns  →  enter  order  #  and  email.    ●  Select  “Refund”  or  “Exchange.”    ●  Print  prepaid  label;  pack  securely.  Multiple  items  can  share  one  box.   Instant  Exchange  Hold    We  place  a  temporary  $1  authorization  on  your  card  and  ship  the  new  size  once  the  carrier  scans  the  return  label.  Authorization  drops  when  our  returns  team  receives  the  original  item.   Return  Window  Exceptions    ●  Holiday  gift  purchases  made  1  Nov  –  31  Dec:  return  period  extended  to  31  Jan.    ●  “Final  Sale”  and  custom-embroidered  items:  no  return  unless  defective.   Refund  Timeline    ●  Warehouse  receipt  →  inspection  ≤  3  business  days  →  refund  initiates.    ●  Stripe  /  Apple  Pay  5-7  banking  days;  PayPal  instant;  Shop-Pay  up  to  10.   Defect  &  Warranty  Claims  (12  months)    ●  Manufacturing  defects  (seam  failure,  sole  separation,  zipper  loss)  are  covered  ●  for  one  year.  Choose  **“Warranty  Claim”**  in  the  portal—upload  3  photos  showing  ●  the  defect.  Approved  claims  ship  replacement  next  business  day.   Footwear  Fit  Exchange  Matrix   |  Current  Size  |  Return  Label  Cost  |  Replacement  Ships  |  Notes  |  |--------------|------------------|-------------------|-------|  |  Too  small     |  Free              |  +0.5  US  size       |  Width  unchanged  |  |  Too  large     |  Free              |  –0.5  US  size       |  EU  size  per  chart  |   Gift  Returns'),\n",
       "  Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Return_and_exchange_policy', 'source': 'data/Everstorm_Return_and_exchange_policy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Gifts  return  for  **store  credit**  emailed  to  the  recipient;  original  purchaser  is  not  notified.   International  Returns    ●  Label  cost  is  deducted  from  refund  (approx.  $15–$25).  Duties  &  taxes  are  ●  non-refundable—claim  with  your  local  customs  office.   Unauthorized  /  Heavily  Worn  Items    ●  Items  showing  heavy  wear  (mud,  pet  hair,  smoke)  will  be  shipped  back  or  ●  donated  to  charity  at  our  discretion  minus  a  $15  handling  fee.   12\\u2002Contact    ●  returns@everstorm.example\\u2002•\\u2002Photo  hotline  +1  (406)  555-0188')],\n",
       " [Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Product_sizing_and_care_guide', 'source': 'data/Everstorm_Product_sizing_and_care_guide.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Everstorm  Outfitters    PRODUCT  SIZING  &  CARE  GUIDE    Rev  2.1  —  18  May  2025   A\\u2002Apparel  Size  Charts  (inches)   |  Unisex  Tee  |  Chest  |  Body  Length  |  |------------|-------|-------------|  |  XS          |  34     |  27           |  |  S           |  36     |  28           |  |  M           |  40     |  29           |  |  L           |  44     |  30           |  |  XL          |  48     |  31           |  |  XXL         |  52     |  32           |   Fit  note:  Tees  are  athletic-cut;  size  up  for  a  relaxed  fit.   B\\u2002Outerwear  Measurements   |  Jacket  Size  |  Chest  (in)  |  Sleeve  (from  center  back)  |  |-------------|-----------|---------------------------|  |  S            |  37–39      |  33                         |  |  M            |  39–42      |  34                         |  |  L            |  42–45      |  35                         |  |  XL           |  45–48      |  36                         |   C\\u2002Footwear  Conversion   |  US  Men  |  US  Women  |  EU   |  UK  |  |--------|----------|-----|----|  |  7       |  8.5       |  40   |  6   |  |  8       |  9.5       |  41   |  7   |  |  9       |  10.5      |  42   |  8   |  |  10      |  11.5      |  43   |  9   |  |  11      |  12.5      |  44.5|  10  |   D\\u2002Fabric  Glossary    •  **Merino  200**  —  200  g/m²  18.9  µm  wool,  odor-resistant,  machine-wash  cold.    •  **Trail-Stretch™**  —  92  %  recycled  nylon,  8  %  spandex,  DWR  coated.    •  **StormCell-3L**  —  3-layer  waterproof/breathable  (20  k  /  20  k).   E\\u2002Care  Instructions  by  Category   |  Item  Type  |  Wash  |  Dry  |  Iron  |  Notes  |'),\n",
       "  Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Product_sizing_and_care_guide', 'source': 'data/Everstorm_Product_sizing_and_care_guide.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='|-----------|------|-----|------|-------|  |  Merino  base  layer  |  Cold  gentle,  wool-safe  detergent  |  Flat  dry  |  Low  |  Avoid  fabric  softener  |  |  DWR  shell  |  Machine  cold,  no  powder  soap  |  Tumble  low  10  min,  then  hang  |  N/A  |  Re-spray  \\nDWR\\n \\nevery\\n \\n10\\n \\nwashes\\n \\n|\\n |  Canvas  pack  |  Hand-wipe  mild  soap  |  Air  |  N/A  |  Remove  frame  sheet  first  |  |  Leather  boots  |  Brush  dirt  dry;  damp  cloth  |  Air  |  N/A  |  Condition  every  3  months  |   F\\u2002DIY  Repair  Tips    •  Seam  separation  ≤  3  cm  →  clean  area,  apply  Gear-Aid  SeamGrip,  24  h  cure.    •  Missing  jacket  snap  →  email  parts@everstorm.example  for  snap-kit  (free).    •  Boot  lace  break  →  63  in  paracord  replacement,  tie  surgeon’s  knot.   G\\u2002Lifetime  Repair  Credit    We  partner  with  **Renew  Workshop**.  Submit  a  ticket  for  a  prepaid  label;  receive  store  credit  equal  to  repair  cost  if  you  opt  to  buy  new  instead  of  repairing.   Questions?  sizecare@everstorm.example')],\n",
       " [Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Shipping_and_Delivery_Policy', 'source': 'data/Everstorm_Shipping_and_Delivery_Policy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Everstorm  Outfitters    SHIPPING  &  DELIVERY  POLICY    Revision  4.0  —  Effective  18  May  2025   1\\u2002Who  We  Ship  To    •  United  States,  Canada,  EU/EEA,  UK,  Australia,  New  Zealand,  Japan,  Singapore    •  No  PO  boxes,  freight  forwarders,  or  sanctioned  destinations  (OFAC  list).   2\\u2002Fulfilment  Centers    •  Reno,  NV  (US  West)\\u2003•  Harrisburg,  PA  (US  East)\\u2003•  Rotterdam,  NL  (EU)    Orders  route  automatically  to  the  closest  node  with  stock.   3\\u2002Processing  Times    Mon–Fri  orders  placed  before  14:00  local  warehouse  time  ship  the  same  day.    Weekend  orders  ship  Monday,  except  public  holidays.   4\\u2002Service  Levels  &  Transit  Targets   |  Region        |  Standard  (Business  Days)  |  Expedited  |  Carrier  |  |--------------|--------------------------|-----------|---------|  |  West  Coast  US|  2  –  3                     |  1          |  UPS  Ground  /  Air  Saver  |  |  Midwest  /  TX  |  3  –  4                     |  2          |  UPS  Ground  /  2-Day  Air  |  |  East  Coast  US|  4  –  5                     |  2          |  UPS  Ground  /  2-Day  Air  |  |  Canada        |  5  –  8                     |  3          |  DHL  Express  /  Canada  Post  |  |  EU,  UK        |  5  –  8                     |  3          |  DHL  Express  /  DPD  |  |  AU  /  NZ       |  7  –  10                    |  4          |  FedEx  Int’l  Priority  |  |  Japan  /  SG    |  4  –  6                     |  2          |  DHL  Express  |   5\\u2002Shipping  Rates  (USD)   |  Basket  Total  |  USA  Std  |  USA  Exp  |  Canada  Std  |  EU  Std*  |  |--------------|---------|---------|------------|---------|  |  <$75          |  7.95     |  live-rate|  14.95      |  €9.95    |  |  ≥$75          |  **FREE**|  live-rate|  **FREE**   |  **FREE**|  \\\\*EU  standard  ships  DDP  (duties  paid).  Other  regions  ship  DDU  (duties  unpaid).   6\\u2002Order  Tracking    A  tracking  link  is  emailed  upon  label  creation.  Status  updates  may  take  up  to  12  h  to  appear  after  the  parcel  is  scanned  at  origin  terminal.   7\\u2002Changing  Your  Address    Contact  logistics@everstorm.example  within  30  min  of  order  placement.  Once  the  parcel  is  “Manifested,”  we  can  no  longer  reroute.'),\n",
       "  Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Shipping_and_Delivery_Policy', 'source': 'data/Everstorm_Shipping_and_Delivery_Policy.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='8\\u2002Lost,  Stolen,  or  Damaged  Parcels    a)  Marked  “Delivered”  but  not  received  →  wait  24  h;  check  neighbors;  then  email  claims@everstorm.example.    b)  No  tracking  movement  for  7  days  (domestic)  /  14  days  (international)  →  we  open  a  carrier  trace.    c)  Visible  transit  damage  →  photograph  box  before  unsealing;  attach  pictures  to  claim  email.   9\\u2002Weather  &  Force  Majeure    Extreme  weather  may  extend  transit  times  beyond  the  targets  above.  Everstorm  is  not  liable  for  carrier  delays  due  to  natural  disasters  but  will  refund  paid  expedited  fees  if  delivery  misses  the  expedited  commitment  by  >24  h.   10\\u2002Holiday  Shipping  Cut-offs  (2025)    •  Hanukkah  /  Christmas  Eve  delivery,  U.S.  Standard  →  order  by  13  Dec  14:00  PT    •  U.S.  Expedited  →  order  by  20  Dec  14:00  PT    •  International  Express  →  order  by  15  Dec  14:00  PT   11\\u2002Contact    logistics@everstorm.example\\u2002•\\u2002+1  (406)  555-0199\\u2002•\\u2002Live  chat  08:00–18:00  MT')],\n",
       " [Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Payment_refund_and_security', 'source': 'data/Everstorm_Payment_refund_and_security.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='FAQs  \\n Which  payment  methods  do  you  accept?   Visa,  MasterCard,  AmEx,  Discover,  Apple  Pay,  Google  Pay,  PayPal,  Shop  Pay.  Installments  (US  \\nonly),\\n \\nKlarna\\n \\nPay-in-4\\n \\n(selected\\n \\nEU\\n \\ncountries).\\n  What  is  3-D  Secure  and  why  did  I  see  a  pop-up?    3-D  Secure  (also  “Verified  by  Visa”  /  “Mastercard  Identity  Check”)  adds  an  extra  one-time  code  \\nfor\\n \\nEU\\n \\nPSD2\\n \\ncompliance.\\n \\nYour\\n \\nbank\\n \\ncontrols\\n \\nthat\\n \\npop-up.\\n  Is  my  data  safe?   All  checkout  traffic  uses  TLS  1.3.  We  never  store  full  card  numbers.  Our  store  is  Level  1  \\nPCI-DSS\\n \\ncompliant.\\n  How  long  do  refunds  take?    We  issue  refunds  the  same  day  your  return  is  scanned.  Banks  typically  post  credit  within:    ●  Card:  5-7  banking  days  ●  PayPal:  immediate  ●  Klarna:  2-5  days    Shop  Pay  Installments  voids  remaining  payments  or  refunds  for  completed  ones.   Where’s  my  Klarna  statement?    Klarna  emails  the  payment  schedule  shortly  after  order  confirmation.  If  not  received,  log  in  at  \\nklarna.com\\n \\nor\\n \\ntheir\\n \\nmobile\\n \\napp.\\n  Why  was  my  card  declined?    Common  reasons:  ZIP  mismatch,  CVV  typo,  foreign  card  blocked  for  cross-border.  Retry  or  call  \\nyour\\n \\nbank;\\n \\nwe\\n \\ncannot\\n \\noverride\\n \\ndeclines.\\n  Do  you  charge  sales  tax?    Yes.  calculated  automatically  in  42  US  states  +  DC  and  applicable  provinces.    Can  I  split  the  payment  across  cards?    Not  at  checkout,  but  buy  a  digital  gift  card  first,  then  apply  it  and  pay  the  balance  with  the  \\nsecond\\n \\ncard.\\n  Crypto  payments?   We  accept  USDC  on  Ethereum  Mainnet  for  orders  above  $500.  Choose  “Coinbase  Commerce”  \\nat\\n \\ncheckout;\\n \\nrate\\n \\nlocked\\n \\nfor\\n \\n15\\n \\nmin.\\n  Need  more  help?  billing@everstorm.example'),\n",
       "  Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Payment_refund_and_security', 'source': 'data/Everstorm_Payment_refund_and_security.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "raw_docs = []\n",
    "\n",
    "raw_docs = [PyPDFLoader(path).load() for path in pdf_paths]\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")\n",
    "raw_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f135e",
   "metadata": {},
   "source": [
    "### (Optional) 2.1 - Load web pages\n",
    "You can also pull content straight from the web. Various libraries support reading and parsing web pages directly into text, which is useful for building custom knowledge bases. One example is **UnstructuredURLLoader** from LangChain, which can extract readable content from raw HTML pages and return them in a structured format. To learn more, see: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html\n",
    "\n",
    "To practice, load each HTML page below and store the results in a list called `raw_docs`. We’ve included a few sample URLs, but you can replace them with any links you prefer.\n",
    "\n",
    "For robustness, add an offline fallback in case a URL fails. In real projects, we typically cache fetched pages to disk, handle rate limits, and track fetch timestamps so content can be refreshed periodically without relying on live network calls during development. For this project, we don’t have offline HTML copies available, but you can still practice by loading any PDFs from the data/ folder using PyPDFLoader and appending them to raw_docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 2[Document(metadata={'source': 'https://developer.bigcommerce.com/docs/store-operations/shipping/TERM.md'}, page_content='Page Not Found\\n\\nಥ_ಥ\\n\\nIf you feel like there should be a page here, please let us know!\\n\\nClick a button below to report the 404 or to return home.'), Document(metadata={'source': 'https://developer.bigcommerce.com/docs/store-operations/orders/refunds'}, page_content='Docs\\n\\nStore operations\\n\\nOrders\\n\\nRefunds\\n\\nOrder Refunds\\n\\nOrder V3 exposes endpoints for creating refunds against orders with settled payments. These endpoints are useful when building order management or payment integrations as they make embedding refund functionality directly into the application possible without requiring merchants to return to their BigCommerce control panel.\\n\\nThis article provides an overview of Order\\'s V3 refund capabilities and includes a step-by-step example for creating a single order refund.\\n\\nSingle order refund example\\n\\nRefunding an order consists of two API requests.\\n\\nRequest Operation Endpoint Description 1 POST /v3/orders/{id}/payment_actions/refund_quotes Calculate amounts and get payment methods 2 POST /v3/orders/{id}/payment_actions/refund Create the refund\\n\\nThe example requests in this article use an order with the following properties:\\n\\nProducts: Single product priced at $10.00\\n\\nTax: $0.83\\n\\nShipping: $10.00\\n\\nThe refunded amount will include the shipping, tax, and product cost (a total of $20.83). We will create a refund quote. Then, we will create a refund using the information contained in the create refund quote response.\\n\\nCreating refund quotes\\n\\nA refund quote provides the tax amount, total refund amount, and a list of available payment methods for order refunds.\\n\\nTo create a refund quote, send a POST request to /v3/orders/{order_id}/payment_actions/refund_quotes.\\n\\nExample request: Create a refund quote\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refund_quotes\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",  // Refund a product\\n      \"item_id\": 8,            // Order product ID\\n      \"quantity\": 1,           // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 9,            // Order address ID\\n      \"amount\": 10,            // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"ORDER\",    // Tax-exempt order level refund\\n      \"item_id\": 9,            // Order ID\\n      \"amount\": 1,             // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"FEE\",     // Refund a fee\\n      \"item_id\": 11,          // Fee ID\\n      \"amount\": 1,            // Amount to refund\\n    }\\n  ]\\n}\\n\\nTo get an item_id, make a GET request to v2/orders/{order_id}/products. The returned id value is the item_id needed to create a PRODUCT refund quote. The returned order_address_id value is the item_id needed to create a SHIPPING refund quote. To read more about using the v2/orders/{order_id}/products endpoint, visit List Order Products.\\n\\nTo get a list of orders and their ids, make a request to get all orders.\\n\\nCreating a refund\\n\\nUse the provider_id, the amount, and items from the refund quote to create a refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",  // Refund a product\\n      \"item_id\": 8,            // Order product ID\\n      \"quantity\": 1            // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 9,            // Order address ID\\n      \"amount\": 10             // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"ORDER\",   // Tax-exempt order level refund\\n      \"item_id\": 123,         // Order ID\\n      \"amount\": 1,            // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"FEE\",     // Refund a fee\\n      \"item_id\": 11,          // Fee ID\\n      \"amount\": 1,            // Amount to refund\\n    }\\n  ],\\n  \"payments\": [\\n    {\\n      \"provider_id\": \"braintree\",\\n      \"amount\": 21.83,\\n      \"offline\": false\\n    }\\n  ]\\n}\\n\\nCreating order level refunds\\n\\nTo refund a tax-exempt custom amount at the order level, set item_type to ORDER and specify the amount to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"ORDER\", // Refund a tax-exempt custom amount\\n      \"item_id\": 1234,      // Order ID\\n      \"amount\": 1,          // Amount to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nRefunding shipping and handling\\n\\nTo refund shipping or handling, set item_type to SHIPPING or HANDLING and specify the amount to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 456,          // Order address ID\\n      \"amount\": 1,             // Amount to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nRefunding products and gift wrapping\\n\\nTo refund a product or gift wrapping, set item_type to PRODUCT or GIFT_WRAPPING and specify the quantity to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",       // Refund a product\\n      \"item_id\": 1234,              // Order product ID\\n      \"quantity\": 1,                // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"GIFT_WRAPPING\", // Refund gift wrapping\\n      \"item_id\": 1234,              // Order product ID\\n      \"quantity\": 1,                // Quantity to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nOffline order refunds\\n\\nPayments collected outside of BigCommerce can be marked as offline when creating a refund. Marking payments as offline is a way to keep track of which portions of an order you refunded. However, no funds were collected. If you did not receive payment using BigCommerce, then the funds can not be refunded directly to the payment source using BigCommerce\\'s refund endpoints.\\n\\nFAQ\\n\\nIs it possible to create a refund without using an item from the order?\\n\\nYes. Set item_type to ORDER and specify an amount to refund. For more information, see create order level refunds.\\n\\nCan I just skip creating the quote and go straight to processing a refund?\\n\\nIt is possible to process a refund without creating a quote first. Quotes serve to make sure you are refunding to the correct payment provider in the correct amount.\\n\\nWhere do I find the item_id?\\n\\nUse the V2 Orders Endpoint to get the required ID:\\n\\nPRODUCT -- Order Product ID\\n\\nGIFT_WRAPPING -- Order Product ID\\n\\nSHIPPING -- Order Address ID\\n\\nHANDLING -- Order Address ID\\n\\nORDER -- Order ID\\n\\nWill this trigger an email to the shopper?\\n\\nYes, if you configure the store to send an email when an order status changes to Refunded or Partially Refunded.\\n\\nHow do I get a list of provider_ids?\\n\\nThe POST Refund Quote exposes provider_ids.\\n\\nWill a refunded item be returned to inventory?\\n\\nNo, you cannot return items to inventory that you refunded via API. You can either update the inventory directly on the product\\'s page or use the Control Panel (opens in a new tab) to change the inventory.\\n\\nTroubleshooting\\n\\nYou must receive payment of the order before you can issue a refund.\\n\\nIf you use a payment gateway, it must support refunds. For a list of payment gateways that support refunds through BigCommerce, see the Supported Payment Gateways section in Processing Refunds (opens in a new tab).\\n\\nIf you use a payment gateway, you must settle payments. Some gateways require a certain amount of time to pass before you can process refunds.\\n\\nRelated resources\\n\\nArticles\\n\\nOrders Overview\\n\\nOrder Webhook Events\\n\\nEndpoints\\n\\nOrders V2 Reference\\n\\nOrders V3 Reference\\n\\nDid you find what you were looking for?\\n\\nOverviewCustomizing printed invoices')] documents from the web.\n",
      "Loaded 2 offline documents. [Document(metadata={'source': 'https://developer.bigcommerce.com/docs/store-operations/shipping/TERM.md'}, page_content='Page Not Found\\n\\nಥ_ಥ\\n\\nIf you feel like there should be a page here, please let us know!\\n\\nClick a button below to report the 404 or to return home.'), Document(metadata={'source': 'https://developer.bigcommerce.com/docs/store-operations/orders/refunds'}, page_content='Docs\\n\\nStore operations\\n\\nOrders\\n\\nRefunds\\n\\nOrder Refunds\\n\\nOrder V3 exposes endpoints for creating refunds against orders with settled payments. These endpoints are useful when building order management or payment integrations as they make embedding refund functionality directly into the application possible without requiring merchants to return to their BigCommerce control panel.\\n\\nThis article provides an overview of Order\\'s V3 refund capabilities and includes a step-by-step example for creating a single order refund.\\n\\nSingle order refund example\\n\\nRefunding an order consists of two API requests.\\n\\nRequest Operation Endpoint Description 1 POST /v3/orders/{id}/payment_actions/refund_quotes Calculate amounts and get payment methods 2 POST /v3/orders/{id}/payment_actions/refund Create the refund\\n\\nThe example requests in this article use an order with the following properties:\\n\\nProducts: Single product priced at $10.00\\n\\nTax: $0.83\\n\\nShipping: $10.00\\n\\nThe refunded amount will include the shipping, tax, and product cost (a total of $20.83). We will create a refund quote. Then, we will create a refund using the information contained in the create refund quote response.\\n\\nCreating refund quotes\\n\\nA refund quote provides the tax amount, total refund amount, and a list of available payment methods for order refunds.\\n\\nTo create a refund quote, send a POST request to /v3/orders/{order_id}/payment_actions/refund_quotes.\\n\\nExample request: Create a refund quote\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refund_quotes\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",  // Refund a product\\n      \"item_id\": 8,            // Order product ID\\n      \"quantity\": 1,           // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 9,            // Order address ID\\n      \"amount\": 10,            // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"ORDER\",    // Tax-exempt order level refund\\n      \"item_id\": 9,            // Order ID\\n      \"amount\": 1,             // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"FEE\",     // Refund a fee\\n      \"item_id\": 11,          // Fee ID\\n      \"amount\": 1,            // Amount to refund\\n    }\\n  ]\\n}\\n\\nTo get an item_id, make a GET request to v2/orders/{order_id}/products. The returned id value is the item_id needed to create a PRODUCT refund quote. The returned order_address_id value is the item_id needed to create a SHIPPING refund quote. To read more about using the v2/orders/{order_id}/products endpoint, visit List Order Products.\\n\\nTo get a list of orders and their ids, make a request to get all orders.\\n\\nCreating a refund\\n\\nUse the provider_id, the amount, and items from the refund quote to create a refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",  // Refund a product\\n      \"item_id\": 8,            // Order product ID\\n      \"quantity\": 1            // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 9,            // Order address ID\\n      \"amount\": 10             // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"ORDER\",   // Tax-exempt order level refund\\n      \"item_id\": 123,         // Order ID\\n      \"amount\": 1,            // Amount to refund\\n    },\\n    {\\n      \"item_type\": \"FEE\",     // Refund a fee\\n      \"item_id\": 11,          // Fee ID\\n      \"amount\": 1,            // Amount to refund\\n    }\\n  ],\\n  \"payments\": [\\n    {\\n      \"provider_id\": \"braintree\",\\n      \"amount\": 21.83,\\n      \"offline\": false\\n    }\\n  ]\\n}\\n\\nCreating order level refunds\\n\\nTo refund a tax-exempt custom amount at the order level, set item_type to ORDER and specify the amount to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"ORDER\", // Refund a tax-exempt custom amount\\n      \"item_id\": 1234,      // Order ID\\n      \"amount\": 1,          // Amount to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nRefunding shipping and handling\\n\\nTo refund shipping or handling, set item_type to SHIPPING or HANDLING and specify the amount to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"SHIPPING\", // Refund shipping\\n      \"item_id\": 456,          // Order address ID\\n      \"amount\": 1,             // Amount to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nRefunding products and gift wrapping\\n\\nTo refund a product or gift wrapping, set item_type to PRODUCT or GIFT_WRAPPING and specify the quantity to refund.\\n\\nExample request: Create a refund\\n\\nPOST https://api.bigcommerce.com/stores/{{STORE_HASH}}/v3/orders/{order_id}/payment_actions/refunds\\nX-Auth-Token: {{ACCESS_TOKEN}}\\nContent-Type: application/json\\nAccept: application/json\\n \\n{\\n  \"order_id\": 1234,\\n  \"items\": [\\n    {\\n      \"item_type\": \"PRODUCT\",       // Refund a product\\n      \"item_id\": 1234,              // Order product ID\\n      \"quantity\": 1,                // Quantity to refund\\n    },\\n    {\\n      \"item_type\": \"GIFT_WRAPPING\", // Refund gift wrapping\\n      \"item_id\": 1234,              // Order product ID\\n      \"quantity\": 1,                // Quantity to refund\\n    }\\n  ],\\n  \"payments\": [...]\\n}\\n\\nOffline order refunds\\n\\nPayments collected outside of BigCommerce can be marked as offline when creating a refund. Marking payments as offline is a way to keep track of which portions of an order you refunded. However, no funds were collected. If you did not receive payment using BigCommerce, then the funds can not be refunded directly to the payment source using BigCommerce\\'s refund endpoints.\\n\\nFAQ\\n\\nIs it possible to create a refund without using an item from the order?\\n\\nYes. Set item_type to ORDER and specify an amount to refund. For more information, see create order level refunds.\\n\\nCan I just skip creating the quote and go straight to processing a refund?\\n\\nIt is possible to process a refund without creating a quote first. Quotes serve to make sure you are refunding to the correct payment provider in the correct amount.\\n\\nWhere do I find the item_id?\\n\\nUse the V2 Orders Endpoint to get the required ID:\\n\\nPRODUCT -- Order Product ID\\n\\nGIFT_WRAPPING -- Order Product ID\\n\\nSHIPPING -- Order Address ID\\n\\nHANDLING -- Order Address ID\\n\\nORDER -- Order ID\\n\\nWill this trigger an email to the shopper?\\n\\nYes, if you configure the store to send an email when an order status changes to Refunded or Partially Refunded.\\n\\nHow do I get a list of provider_ids?\\n\\nThe POST Refund Quote exposes provider_ids.\\n\\nWill a refunded item be returned to inventory?\\n\\nNo, you cannot return items to inventory that you refunded via API. You can either update the inventory directly on the product\\'s page or use the Control Panel (opens in a new tab) to change the inventory.\\n\\nTroubleshooting\\n\\nYou must receive payment of the order before you can issue a refund.\\n\\nIf you use a payment gateway, it must support refunds. For a list of payment gateways that support refunds through BigCommerce, see the Supported Payment Gateways section in Processing Refunds (opens in a new tab).\\n\\nIf you use a payment gateway, you must settle payments. Some gateways require a certain amount of time to pass before you can process refunds.\\n\\nRelated resources\\n\\nArticles\\n\\nOrders Overview\\n\\nOrder Webhook Events\\n\\nEndpoints\\n\\nOrders V2 Reference\\n\\nOrders V3 Reference\\n\\nDid you find what you were looking for?\\n\\nOverviewCustomizing printed invoices')]\n"
     ]
    }
   ],
   "source": [
    "URLS = [\n",
    "    # --- BigCommerce – shipping & refunds ---\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/shipping/TERM.md\",\n",
    "    \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "    # --- Stripe – disputes & chargebacks ---\n",
    "    # \"https://docs.stripe.com/disputes\",  \n",
    "    # --- WooCommerce – REST API reference ---\n",
    "    # \"https://woocommerce.github.io/woocommerce-rest-api-docs/v3.html\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    ########################\n",
    "    #### Your code here (~2-3 lines of code) ####\n",
    "    ########################\n",
    "    URL_loader = UnstructuredURLLoader(urls=URLS)\n",
    "    raw_docs = URL_loader.load()\n",
    "    print(f\"Fetched {len(raw_docs)}{raw_docs} documents from the web.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"⚠️  Web fetch failed, using offline copies:\", e)\n",
    "    raw_docs = []\n",
    "    ########################\n",
    "    #raw_docs = [PyPDFLoader(path).load() for path in pdf_paths]    ########################   \n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "    # Put any fallback files under ./offline_docs (e.g., .md, .txt, .html)\n",
    "patterns = [\"**/*.md\", \"**/*.txt\", \"**/*.html\"]\n",
    "for pattern in patterns:\n",
    "    try:\n",
    "        loader = DirectoryLoader(\n",
    "            \"offline_docs\",\n",
    "            glob=pattern,\n",
    "            loader_cls=TextLoader,\n",
    "            show_progress=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        raw_docs.extend(loader.load())\n",
    "    except Exception:\n",
    "        # Skip unreadable pattern/file types quietly\n",
    "        pass\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} offline documents. {raw_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ac4490",
   "metadata": {},
   "source": [
    "### 2.2 - Chunk the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c242e44",
   "metadata": {},
   "source": [
    "Long documents won’t work well directly with most LLMs. They can easily exceed the model’s context window, making it impossible for the model to read or reason over the full text at once. Even if they fit, processing long inputs can be inefficient and lead to weaker retrieval results.\n",
    "\n",
    "To handle this, we split large documents into smaller, overlapping chunks. Several libraries can help with text splitting, each designed to preserve structure or balance chunk size. A popular choice is `RecursiveCharacterTextSplitter` from LangChain, which splits text intelligently while keeping paragraph or sentence boundaries intact. To familiarize youself with the library, visit: https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html\n",
    "\n",
    "In this project, we’ll split each document into chunks of roughly 300 tokens with a 30-token overlap using `RecursiveCharacterTextSplitter`. This overlap helps maintain continuity across chunks while ensuring each piece stays small enough for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d48a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 40 chunks ready for embeddingpage_content='Docs\n",
      "\n",
      "Store operations\n",
      "\n",
      "Orders\n",
      "\n",
      "Refunds\n",
      "\n",
      "Order Refunds' metadata={'source': 'https://developer.bigcommerce.com/docs/store-operations/orders/refunds'}\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "print(f\"✅ {len(chunks)} chunks ready for embedding{chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89e6213",
   "metadata": {},
   "source": [
    "## 3 -Build a retriever\n",
    "\n",
    "A *retriever* lets the RAG pipeline efficiently look up small, relevant pieces of context at query‑time. This step has two parts:\n",
    "1. **Load a model to generate embeddings**: convert each text chunk from the reference documents into a fixed‑length vector that captures its semantic meaning.  \n",
    "2. **Build vector database**: store these embeddings in a vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90db2d2",
   "metadata": {},
   "source": [
    "### 3.1 - Load a model to generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da722b3b",
   "metadata": {},
   "source": [
    "The goal of this step is to convert each document chunk into a numerical vector (an embedding) that captures its semantic meaning. These embeddings allow our retriever to find and compare similar pieces of text efficiently.\n",
    "\n",
    "There are models trained specifically for this purpose, called embedding models. One popular example is OpenAI’s `text-embedding-3-small`, which produces high-quality embeddings that work well for retrieval and semantic search.\n",
    "\n",
    "If you prefer running everything locally, you can use smaller open-source models such as `gte-small` (77 M parameters). These local models load quickly, don’t require internet access, and are ideal for experimentation or environments without API access. However, they’re typically less powerful than hosted models.\n",
    "\n",
    "Alternatively, you can connect to an API service to access stronger models like OpenAI’s. These require setting an API key (for example, OPENAI_API_KEY) in your environment. OpenAI allows you to create a free account and sometimes offers limited trial credits for new users, but ongoing access requires a billing setup. \n",
    "\n",
    "In this exercise, we’ll stick to the smaller gte-small model for simplicity and reproducibility. We'll use our imported `SentenceTransformerEmbeddings` library to load the model and use it to embed queries. To learn more about lagnchain's embedding support, visit: https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a40600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a222122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.03405226767063141,\n",
       " -0.029402822256088257,\n",
       " 0.0217142291367054,\n",
       " -0.006029120646417141,\n",
       " 0.03775044530630112,\n",
       " 0.026207849383354187,\n",
       " 0.0015799491666257381,\n",
       " 0.053875453770160675,\n",
       " 0.005376556422561407,\n",
       " 0.015529812313616276,\n",
       " 0.008030730299651623,\n",
       " -0.0361722856760025,\n",
       " 0.06260193884372711,\n",
       " 0.057945750653743744,\n",
       " 0.012517298571765423,\n",
       " -0.008481158874928951,\n",
       " -0.009156469255685806,\n",
       " 0.06365834921598434,\n",
       " -0.07545200735330582,\n",
       " 0.012592596933245659,\n",
       " 0.034156493842601776,\n",
       " -0.04223155230283737,\n",
       " -0.0160830020904541,\n",
       " -0.06224709376692772,\n",
       " 0.012022402137517929,\n",
       " 0.024824485182762146,\n",
       " -0.06751365214586258,\n",
       " -0.004868444055318832,\n",
       " 0.0038430215790867805,\n",
       " -0.23420697450637817,\n",
       " 0.012040655128657818,\n",
       " -0.09038142114877701,\n",
       " 0.07782766968011856,\n",
       " -0.029737552627921104,\n",
       " 0.017970260232686996,\n",
       " -0.028921818360686302,\n",
       " -0.012647110037505627,\n",
       " 0.0268105436116457,\n",
       " -0.015985552221536636,\n",
       " 0.058696191757917404,\n",
       " 0.034959226846694946,\n",
       " -0.027735644951462746,\n",
       " -0.01973912864923477,\n",
       " -0.06919819116592407,\n",
       " -0.02184997871518135,\n",
       " -0.03596764802932739,\n",
       " -0.03316784277558327,\n",
       " -0.02840879000723362,\n",
       " 0.0036035836674273014,\n",
       " -0.007499413564801216,\n",
       " 0.019179178401827812,\n",
       " -0.004103024955838919,\n",
       " -0.025208987295627594,\n",
       " 0.02541108801960945,\n",
       " 0.03963221237063408,\n",
       " 0.09404708445072174,\n",
       " 0.07558103650808334,\n",
       " 0.01664513349533081,\n",
       " -0.00982098001986742,\n",
       " 0.05590839311480522,\n",
       " 0.03328106924891472,\n",
       " 0.00873635709285736,\n",
       " -0.1498326063156128,\n",
       " 0.08543387800455093,\n",
       " 0.007432654965668917,\n",
       " 0.00842816848307848,\n",
       " -0.03569746017456055,\n",
       " 0.02376396767795086,\n",
       " 0.030721647664904594,\n",
       " 0.04037866368889809,\n",
       " -0.043920744210481644,\n",
       " -0.006920634303241968,\n",
       " 0.024886557832360268,\n",
       " 0.05307811126112938,\n",
       " 1.62624546646839e-05,\n",
       " -0.004741786513477564,\n",
       " 0.05676154047250748,\n",
       " -0.02505762316286564,\n",
       " 0.03519930690526962,\n",
       " -0.0079483762383461,\n",
       " 0.0046741655096411705,\n",
       " -0.004728527739644051,\n",
       " 0.010728164575994015,\n",
       " 0.017163235694169998,\n",
       " -0.01699075847864151,\n",
       " -0.03195915371179581,\n",
       " 0.03731853887438774,\n",
       " -0.05331098660826683,\n",
       " 0.027327541261911392,\n",
       " 0.0031567297410219908,\n",
       " -0.034759312868118286,\n",
       " -0.0399584136903286,\n",
       " -0.006732406560331583,\n",
       " 0.00930771417915821,\n",
       " -0.0810280591249466,\n",
       " -0.04608633369207382,\n",
       " 0.06797268986701965,\n",
       " 0.020271770656108856,\n",
       " -0.04292967543005943,\n",
       " 0.2257390320301056,\n",
       " -0.0453256219625473,\n",
       " 0.018381938338279724,\n",
       " 0.06888723373413086,\n",
       " -0.0625983476638794,\n",
       " 0.0084441639482975,\n",
       " -0.0265274029225111,\n",
       " -0.04604315757751465,\n",
       " -0.016690300777554512,\n",
       " -0.04067646339535713,\n",
       " -0.019202347844839096,\n",
       " -0.008135323412716389,\n",
       " -0.04040995240211487,\n",
       " 0.020604506134986877,\n",
       " -0.04897438734769821,\n",
       " 0.07434501498937607,\n",
       " -0.038201577961444855,\n",
       " 0.04950385540723801,\n",
       " 0.002803182927891612,\n",
       " 0.0024603281635791063,\n",
       " -0.01871779002249241,\n",
       " 0.00445222994312644,\n",
       " 0.0238014105707407,\n",
       " 0.032870687544345856,\n",
       " -0.009753633290529251,\n",
       " 0.04500085487961769,\n",
       " -0.035597652196884155,\n",
       " 0.08233574777841568,\n",
       " 0.10841337591409683,\n",
       " 0.04459362104535103,\n",
       " 0.0774565115571022,\n",
       " 0.041218746453523636,\n",
       " 0.03572962433099747,\n",
       " -0.08606616407632828,\n",
       " 0.0031058616004884243,\n",
       " -0.019382290542125702,\n",
       " 0.05285954475402832,\n",
       " -0.015084054321050644,\n",
       " 0.006388021167367697,\n",
       " -0.003255956806242466,\n",
       " -0.05007922276854515,\n",
       " -0.01730000227689743,\n",
       " -0.032521434128284454,\n",
       " 0.012966761365532875,\n",
       " -0.09756186604499817,\n",
       " -0.04401351138949394,\n",
       " 0.1000383049249649,\n",
       " -0.05893014743924141,\n",
       " -0.003757895901799202,\n",
       " -0.020995033904910088,\n",
       " 0.003334876848384738,\n",
       " -0.026146193966269493,\n",
       " 0.02453327551484108,\n",
       " -0.024176904931664467,\n",
       " -0.025529799982905388,\n",
       " 0.032670632004737854,\n",
       " 0.02307673916220665,\n",
       " -0.0001637365494389087,\n",
       " 0.04670072719454765,\n",
       " -0.03748241811990738,\n",
       " 0.009293166920542717,\n",
       " -0.02300797775387764,\n",
       " -0.040277138352394104,\n",
       " -0.023135917261242867,\n",
       " 0.07661605626344681,\n",
       " -0.0026938843075186014,\n",
       " -0.10472002625465393,\n",
       " -0.025787532329559326,\n",
       " -0.013543285429477692,\n",
       " 0.05234140902757645,\n",
       " -0.050041500478982925,\n",
       " 0.060727741569280624,\n",
       " -0.0001384513161610812,\n",
       " -0.030639220029115677,\n",
       " 0.03744061663746834,\n",
       " 0.10781801491975784,\n",
       " 0.030287640169262886,\n",
       " -0.06890667229890823,\n",
       " 0.003310972824692726,\n",
       " 0.04126909747719765,\n",
       " 0.007883445359766483,\n",
       " 0.035124413669109344,\n",
       " -0.061473436653614044,\n",
       " -0.050178162753582,\n",
       " 0.04769586771726608,\n",
       " 0.018367310985922813,\n",
       " -0.06202108412981033,\n",
       " -0.0038231490179896355,\n",
       " -0.04118669405579567,\n",
       " 0.03764618933200836,\n",
       " 0.04733294993638992,\n",
       " -0.01702284812927246,\n",
       " 0.016741905361413956,\n",
       " -0.02860066294670105,\n",
       " -0.05640341714024544,\n",
       " -0.0570373609662056,\n",
       " -0.0107836052775383,\n",
       " 0.0065189809538424015,\n",
       " -0.03264211118221283,\n",
       " 0.04043857008218765,\n",
       " -0.042505960911512375,\n",
       " 0.07306927442550659,\n",
       " 0.014338056556880474,\n",
       " -0.015510303899645805,\n",
       " 0.07052704691886902,\n",
       " -0.003685449017211795,\n",
       " 0.011609998531639576,\n",
       " 0.010982287116348743,\n",
       " -0.031650856137275696,\n",
       " 0.03356824070215225,\n",
       " 0.012387248687446117,\n",
       " -0.04700180143117905,\n",
       " 0.0035565944854170084,\n",
       " 0.02537684701383114,\n",
       " -0.007578270509839058,\n",
       " -0.03110378235578537,\n",
       " 0.028100252151489258,\n",
       " -0.024671971797943115,\n",
       " 0.04753648862242699,\n",
       " 0.027434131130576134,\n",
       " 0.036317553371191025,\n",
       " -0.012709537521004677,\n",
       " -0.10277438908815384,\n",
       " -0.05537065491080284,\n",
       " -0.23417842388153076,\n",
       " -0.008182749152183533,\n",
       " -0.02578936703503132,\n",
       " -0.04474231228232384,\n",
       " 0.01828226074576378,\n",
       " -0.060291022062301636,\n",
       " 0.047020312398672104,\n",
       " -0.013418076559901237,\n",
       " 0.0675121620297432,\n",
       " 0.03325099125504494,\n",
       " 0.08445443212985992,\n",
       " -0.049499791115522385,\n",
       " -0.018067672848701477,\n",
       " -0.015681574121117592,\n",
       " -0.018173716962337494,\n",
       " 0.043014418333768845,\n",
       " 0.026589827612042427,\n",
       " 0.03362099081277847,\n",
       " 0.009517615661025047,\n",
       " -0.025469372048974037,\n",
       " -0.038434647023677826,\n",
       " -0.004931026604026556,\n",
       " 0.01549874059855938,\n",
       " -0.05198296159505844,\n",
       " 0.07844173163175583,\n",
       " -0.0578741729259491,\n",
       " 0.23359894752502441,\n",
       " 0.10440801829099655,\n",
       " 0.03766931965947151,\n",
       " -0.04097166284918785,\n",
       " 0.041099537163972855,\n",
       " 0.015435539186000824,\n",
       " -0.041760362684726715,\n",
       " -0.1613015979528427,\n",
       " 0.07081132382154465,\n",
       " 0.00972155760973692,\n",
       " -0.02292272262275219,\n",
       " 0.009671446867287159,\n",
       " -0.03445037454366684,\n",
       " -0.012923575937747955,\n",
       " -0.02600206807255745,\n",
       " 0.03245783969759941,\n",
       " 0.05358591303229332,\n",
       " -0.05623367428779602,\n",
       " -0.008012034930288792,\n",
       " -0.055106375366449356,\n",
       " -0.08257642388343811,\n",
       " 0.013382512144744396,\n",
       " -0.07231210172176361,\n",
       " 0.037637460976839066,\n",
       " 0.03189253807067871,\n",
       " -0.038971416652202606,\n",
       " 0.03731032460927963,\n",
       " 0.030467627570033073,\n",
       " -0.031029190868139267,\n",
       " -0.04190099239349365,\n",
       " -0.0446375273168087,\n",
       " 0.011319013312458992,\n",
       " -0.042642511427402496,\n",
       " 0.051995519548654556,\n",
       " -0.025726722553372383,\n",
       " -0.05077885836362839,\n",
       " 0.03124419040977955,\n",
       " -0.054840702563524246,\n",
       " 0.040409572422504425,\n",
       " -0.007889056578278542,\n",
       " -0.006150491535663605,\n",
       " -0.02660212107002735,\n",
       " 0.03736574947834015,\n",
       " -0.03443155437707901,\n",
       " -0.03289837762713432,\n",
       " 0.060303956270217896,\n",
       " -0.0017871363088488579,\n",
       " -0.0005226013599894941,\n",
       " 0.0837467834353447,\n",
       " 0.01013264898210764,\n",
       " 0.015596861019730568,\n",
       " -0.006385358050465584,\n",
       " -0.0433327853679657,\n",
       " -0.03720172494649887,\n",
       " 0.013554655015468597,\n",
       " -0.022094765678048134,\n",
       " 0.03746701404452324,\n",
       " 0.05013297125697136,\n",
       " 0.04810549318790436,\n",
       " 0.024617306888103485,\n",
       " 0.041589923202991486,\n",
       " -0.009928248822689056,\n",
       " 0.028504276648163795,\n",
       " -0.022412864491343498,\n",
       " 0.017844177782535553,\n",
       " -0.03549175709486008,\n",
       " -0.0058589172549545765,\n",
       " -0.005164834205061197,\n",
       " 0.010827787220478058,\n",
       " -0.015465298667550087,\n",
       " -0.3149524927139282,\n",
       " 0.06765784323215485,\n",
       " 0.026197725906968117,\n",
       " 0.04503193497657776,\n",
       " -0.02328583598136902,\n",
       " 0.01621124893426895,\n",
       " 0.01627172902226448,\n",
       " -0.004602269269526005,\n",
       " -0.07415158301591873,\n",
       " -0.01603556051850319,\n",
       " 0.02067391574382782,\n",
       " 0.029944662004709244,\n",
       " 0.07321444153785706,\n",
       " -0.010390839539468288,\n",
       " 0.008283969946205616,\n",
       " 0.031793415546417236,\n",
       " 0.015457261353731155,\n",
       " -0.05212867259979248,\n",
       " 0.041329726576805115,\n",
       " -0.044014062732458115,\n",
       " 0.007938431575894356,\n",
       " 0.03531097620725632,\n",
       " 0.20021364092826843,\n",
       " -0.06504353135824203,\n",
       " 0.035102520138025284,\n",
       " -0.016897989436984062,\n",
       " -0.026080479845404625,\n",
       " 0.021585380658507347,\n",
       " 0.045292772352695465,\n",
       " 0.00944281555712223,\n",
       " -0.008015863597393036,\n",
       " 0.021899817511439323,\n",
       " 0.08915343135595322,\n",
       " -0.02527748793363571,\n",
       " 0.02224459871649742,\n",
       " 0.029659194871783257,\n",
       " -0.029371293261647224,\n",
       " -0.004014835227280855,\n",
       " 0.05200834944844246,\n",
       " -0.0025847696233540773,\n",
       " 0.0002816407068166882,\n",
       " 0.009140978567302227,\n",
       " -0.08899489045143127,\n",
       " 0.04538657143712044,\n",
       " 0.039628561586141586,\n",
       " -0.029099248349666595,\n",
       " 0.02838023565709591,\n",
       " -0.015737174078822136,\n",
       " 0.0006883350433781743,\n",
       " 0.013572282157838345,\n",
       " -0.01380071695894003,\n",
       " 0.011001886799931526,\n",
       " -0.01536487601697445,\n",
       " -0.024987852200865746,\n",
       " 0.012716387398540974,\n",
       " 0.03626077622175217,\n",
       " -0.010617206804454327,\n",
       " -0.027318477630615234,\n",
       " -0.013753002509474754,\n",
       " -0.02462717890739441,\n",
       " 0.020618755370378494,\n",
       " -0.06507885456085205,\n",
       " 0.010205205529928207,\n",
       " 0.0028154272586107254,\n",
       " 0.010472508147358894]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings as SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_vector = []\n",
    "\n",
    "# Embed the sentence \"Hello world! and store it in an embedding_vector.\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "embedding_vector = embeddings.embed_query(\"Hello world! and store it in an embedding_vector\")\n",
    "print(len(embedding_vector))\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebdae3d",
   "metadata": {},
   "source": [
    "### 3.2 - Build a vector database\n",
    "\n",
    "Once we have embeddings, we need a way to store and search them efficiently. A simple list wouldn’t scale well, especially when we have thousands of chunks and need to quickly find the most relevant ones.\n",
    "\n",
    "To solve this, we use **FAISS**, an open-source similarity search library developed by Meta. FAISS is optimized for fast nearest-neighbor search in high-dimensional spaces, making it ideal for tasks like semantic retrieval and recommendation. It’s strongly encouraged to visit their quickstart guide to understand how FAISS works and how to use it effectively: https://github.com/facebookresearch/faiss/wiki/getting-started\n",
    "\n",
    "In this step, we’ll feed all our document embeddings into FAISS, which builds an in-memory vector index. This index allows us to efficiently query for the *k* most similar chunks to any given question.\n",
    "\n",
    "During inference, we’ll use this index to retrieve the top-k relevant chunks and pass them to the LLM as context, enabling it to answer questions grounded in our documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store with 40 embeddings created and saved locally!\n"
     ]
    }
   ],
   "source": [
    "# Expected steps:\n",
    "    # 1. Build the FAISS index from the list of document chunks and their embeddings.\n",
    "    # 2. Create a retriever object with a suitable k value (e.g., 8).\n",
    "    # 3. Save the vector store locally (e.g., under \"faiss_index\").\n",
    "    # 4. Print a short confirmation showing how many embeddings were stored.\n",
    "\n",
    "vectordb = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"✅ Vector store with\", vectordb.index.ntotal, \"embeddings\" \" created and saved locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456f820",
   "metadata": {},
   "source": [
    "## 4 - Build the generation engine\n",
    "At the core of any RAG system lies an **LLM**. The retriever finds relevant information, and the LLM uses that information to generate coherent, context-aware responses.\n",
    "\n",
    "In this project, we’ll use **Gemma 3* (1B), a small but capable open-weight model, and run it entirely on your local machine using Ollama. This means you won’t need API keys or internet access to generate responses once the model is downloaded.\n",
    "\n",
    "**What is Ollama?**\n",
    "\n",
    "Ollama is a lightweight runtime for managing and serving open-weight LLMs locally. It provides:\n",
    "* A simple REST API running at localhost:11434, so your code can interact with the model via standard HTTP calls.\n",
    "* A model registry and command-line tool** to pull, run, and manage models easily.\n",
    "* Support for a wide variety of models (e.g., Gemma, Llama, Mistral, Phi, etc.), making it ideal for experimentation.\n",
    "\n",
    "To learn more about Ollama, visit: https://github.com/ollama/ollama. You can browse all supported models and their sizes here: https://ollama.com/library\n",
    "\n",
    "\n",
    "### 4.1 - Install `ollama` and serve `gemma3`\n",
    "\n",
    "Follow these steps to set up Ollama and start the model server:\n",
    "\n",
    "**1 - Install**\n",
    "```bash\n",
    "# macOS (Homebrew)\n",
    "brew install ollama\n",
    "# Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "If you’re on Windows, install using the official installer from https://ollama.com/download.\n",
    "\n",
    "**2 - Start the Ollama server (keep this terminal open)**\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "This command launches a local server at http://localhost:11434, which will stay running in the background.\n",
    "\n",
    "\n",
    "**3 - Pull the Gemma mode (or the model of your choice) in a new terminal**\n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "This downloads the 1B version of Gemma 3, a compact model suitable for running on most modern laptops. Once downloaded, Ollama will automatically handle model loading and caching.\n",
    "\n",
    "\n",
    "After this setup, your system is ready to generate responses locally using the Gemma model through the Ollama API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6a05a",
   "metadata": {},
   "source": [
    "### 4.2 - Test LLM with a random prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here’s a fun fact about the Raspberry Pi:\n",
      "\n",
      "**It was originally designed to be a tiny, low-power computer for scientific research!** They even had a built-in USB port for connecting to scientific equipment! \n",
      "\n",
      "It’s amazing how a little computer could be so versatile and adaptable. 😊\n",
      "\n",
      "Do you want to know another fact?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "print(llm.invoke(\"Tell me one fun fact about the Raspberry Pi.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556263d",
   "metadata": {},
   "source": [
    "## Build a RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfec15",
   "metadata": {},
   "source": [
    "### 5.1 - Define a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5c84b",
   "metadata": {},
   "source": [
    "At this stage, we need to tell the model how to behave when generating answers. The **system prompt** acts as the model’s rulebook. It should clearly instruct the model to answer only using the retrieved context and to admit when it doesn’t know the answer. This helps prevent hallucination and keeps the responses grounded in the provided documents.\n",
    "\n",
    "In general, a good RAG prompt emphasizes three things: stay within context, stay factual, and stay concise. This is important because RAG works by grounding the LLM in retrieved text. If the prompt is vague, the model may invent details. A precise system prompt reduces hallucinations and keeps answers aligned with your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with “I'm not sure from the docs.”\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27cc9a",
   "metadata": {},
   "source": [
    "### 5.2 Create a RAG chain\n",
    "Now that we have a retriever, a prompt, and a language model, we can connect them into a single RAG pipeline. The retriever finds the most relevant chunks from our vector index, the prompt injects those chunks into the system message, and the LLM uses that context to produce the final answer. (retriever → prompt → model)\n",
    "\n",
    "This connection is handled through LangChain’s `ConversationalRetrievalChain`, which combines retrieval and generation. To familiarize yourself with the library, visit: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b138b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYOUR CODE HERE (~3-5 lines of code)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected steps:\n",
    "    # 1. Create a PromptTemplate that uses the SYSTEM_TEMPLATE you defined earlier, with input variables for \"context\" and \"question\".\n",
    "    # 2. Initialize your LLM using Ollama with the gemma3:1b model and a low temperature (e.g., 0.1) for reliable, grounded responses.\n",
    "    # 3. Build a ConversationalRetrievalChain by combining the LLM, the retriever, and your custom prompt and name it \"chain\".\n",
    "\n",
    "#1.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=SYSTEM_TEMPLATE,\n",
    ")\n",
    "\n",
    "#2.\n",
    "llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "\n",
    "#3.\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~3-5 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9db038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a6c7fc",
   "metadata": {},
   "source": [
    "When you ask a question, the retriever pulls the top few relevant text chunks, the model reads them through the system prompt, and then it generates an answer based on that context.\n",
    "\n",
    "This structure makes the system transparent and easy to debug. You can inspect what text was retrieved, tune parameters like k, and experiment with different prompts to see how they affect the output quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1eb44d",
   "metadata": {},
   "source": [
    "### 5.3 - Validate the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ee50de",
   "metadata": {},
   "source": [
    "We run a few questions to make sure everything behaves as expecte. Experiment by adding you own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d7a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: If I'm not happy with my purchase, what is your refund policy and how do I start a return?\n",
      "A: “No, you cannot return items to inventory that you refunded via API. You can either update the inventory directly on the product’s page or use the Control Panel (opens in a new tab) to change the inventory.”\n",
      "\n",
      "“Offline order refunds\n",
      "\n",
      "FAQ\n",
      "\n",
      "Is it possible to create a refund without using an item from the order?\n",
      "\n",
      "Yes. Set item_type to ORDER and specify an amount to refund. For more information, see create order level refunds.”\n",
      "\n",
      "“Can I just skip creating the quote and go straight to processing a refund?”\n",
      "\n",
      "“It is possible to process a refund without creating a quote first. Quotes serve to make sure you are refunding to the correct payment provider in the correct amount.”\n",
      "\n",
      "“Where do I find the item_id?”\n",
      "\n",
      "“Use the V2 Orders Endpoint to get the required ID:\n",
      "\n",
      "PRODUCT -- Order Product ID”\n",
      "\n",
      "“Will a refunded item be returned to inventory?”\n",
      "\n",
      "“USER:\n",
      "If I'm not happy with my purchase, what is your refund policy and how do I start a return?”\n",
      "\n",
      "Q: How long will delivery take for a standard order, and where can I track my package once it ships?\n",
      "A: I'm not sure from the docs.\n",
      "\n",
      "Q: What's the quickest way to contact your support team, and what are your operating hours?\n",
      "A: I'm not sure from the docs.\n",
      "\n",
      "Q: Do you also sell internationally, and are there any extra fees for shipping outside the US?\n",
      "A: I'm not sure from the docs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"If I'm not happy with my purchase, what is your refund policy and how do I start a return?\",\n",
    "    \"How long will delivery take for a standard order, and where can I track my package once it ships?\",\n",
    "    \"What's the quickest way to contact your support team, and what are your operating hours?\",\n",
    "    \"Do you also sell internationally, and are there any extra fees for shipping outside the US?\"\n",
    "]\n",
    "\n",
    "# Expected steps:\n",
    "    # 1. Initialize an empty chat_history list.\n",
    "    # 2. Loop through test_questions, pass each question and the current chat history to the chain, and append the new answer.\n",
    "    # 3. Print each question and the LLM's response to verify it’s working correctly.\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\"\n",
    "#1.\n",
    "chat_history = []\n",
    "for question in test_questions:\n",
    "    #2.\n",
    "    result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "    answer = result[\"answer\"]\n",
    "    chat_history.append((question, answer))\n",
    "    #3.\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8c6b4",
   "metadata": {},
   "source": [
    "### 6 - Build the Streamlit UI (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc640b",
   "metadata": {},
   "source": [
    "The goal here is to create a tiny demo so you can interact with your RAG system. The focus is not on UI design. We will build a very small interface only to demonstrate the end-to-end flow.\n",
    "\n",
    "There are many ways to make a UI. Some frameworks are powerful but take longer to set up, while others are simple and good for quick experiments. Streamlit is a common choice for fast prototyping because it lets you make a usable interface with only a few lines of Python. If you want to learn the basics, see the Streamlit Quickstart: https://docs.streamlit.io/deploy/streamlit-community-cloud/get-started/quickstart\n",
    "\n",
    "This step is optional. If it is not useful for your work, you can skip it. We will also complete this part together during the live session.\n",
    "\n",
    "In this cell, we write a minimal **`app.py`** that starts a simple chat UI and calls your RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c182fb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# ==============================================================================\n",
    "# 0. CONFIGURATION & INITIALIZATION (Cache the heavy lifting)\n",
    "# ==============================================================================\n",
    "\n",
    "# NOTE: Replace 'your_vector_store_path' with the actual path where you saved your FAISS index\n",
    "VECTOR_STORE_PATH = \"faiss_index\",\n",
    "\n",
    "# Define the Prompt Template (Assuming you defined this earlier)\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a helpful and friendly assistant. \n",
    "Use the following context to answer the user's question. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "@st.cache_resource\n",
    "def get_rag_chain():\n",
    "    \"\"\"Initializes and returns the RAG chain.\"\"\"\n",
    "    try:\n",
    "        # Load the Embeddings Model\n",
    "        # Use HuggingFaceEmbeddings from the new package\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "        \n",
    "        # Load the Vector Store\n",
    "        vectorstore = FAISS.load_local(\n",
    "            folder_path=VECTOR_STORE_PATH, \n",
    "            embeddings=embedding_model, \n",
    "            allow_dangerous_deserialization=True # Necessary for loading\n",
    "        )\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "        # Initialize the LLM (Ensure Ollama is running locally!)\n",
    "        llm = Ollama(model=\"gemma3:1b\", temperature=0.1)\n",
    "\n",
    "        # Create the PromptTemplate\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=SYSTEM_TEMPLATE,\n",
    "        )\n",
    "\n",
    "        # Build the ConversationalRetrievalChain (using the corrected method)\n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "        \n",
    "        return chain\n",
    "    \n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading RAG components. Is Ollama running and is '{VECTOR_STORE_PATH}' correct? Error: {e}\")\n",
    "        st.stop()\n",
    "        \n",
    "# Initialize the chain\n",
    "chain = get_rag_chain()\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. STREAMLIT UI SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "st.set_page_config(page_title=\"Prasad's Local RAG Chatbot Demo\", layout=\"wide\")\n",
    "st.title(\"Local Gemma RAG Chatbot (Ollama)\")\n",
    "st.caption(\"Ask me about the documents in the vector store.\")\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "    # Add an initial greeting message\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": \"Hello! How can I assist you with your documents today?\"})\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. USER INPUT AND RAG CHAIN CALL\n",
    "# ==============================================================================\n",
    "\n",
    "if prompt := st.chat_input(\"Your question\"):\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate assistant response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            \n",
    "            # Extract previous chat history for the chain\n",
    "            # The chain expects a list of tuples: [(human_msg, ai_msg), ...]\n",
    "            chat_history = []\n",
    "            for i in range(1, len(st.session_state.messages)):\n",
    "                if st.session_state.messages[i][\"role\"] == \"user\":\n",
    "                    user_msg = st.session_state.messages[i][\"content\"]\n",
    "                    # Find the corresponding AI response (it should be the next message)\n",
    "                    if i + 1 < len(st.session_state.messages) and st.session_state.messages[i + 1][\"role\"] == \"assistant\":\n",
    "                        ai_msg = st.session_state.messages[i + 1][\"content\"]\n",
    "                        chat_history.append((user_msg, ai_msg))\n",
    "\n",
    "            # Call the ConversationalRetrievalChain\n",
    "            result = chain.invoke(\n",
    "                {\"question\": prompt, \"chat_history\": chat_history}\n",
    "            )\n",
    "\n",
    "            response = result[\"answer\"]\n",
    "            source_docs = result[\"source_documents\"]\n",
    "\n",
    "            # Display the main answer\n",
    "            st.markdown(response)\n",
    "\n",
    "            # Optionally, display the source documents\n",
    "            with st.expander(\"Show Sources\"):\n",
    "                for i, doc in enumerate(source_docs):\n",
    "                    st.write(f\"**Source {i+1}**: {doc.metadata.get('source', 'N/A')}\")\n",
    "                    st.caption(doc.page_content[:200] + \"...\") # Show first 200 chars\n",
    "\n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc8ad3",
   "metadata": {},
   "source": [
    "Run `streamlit run app.py` from your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7328a5",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You’ve just built, tested, and demoed a fully working **customer-support chatbot**.  \n",
    "In one project you:\n",
    "\n",
    "* **Prepared policy docs**: loaded and chunked them for fast retrieval.  \n",
    "* **Built a vector store**: created a FAISS index with text embeddings.  \n",
    "* **Plugged in an LLM**: wrapped Gemma3 with LangChain and a prompt-aware RAG chain.  \n",
    "* **Validated end-to-end**: answered refund, shipping, and contact questions with confidence.  \n",
    "\n",
    "Swap in new documents, tweak the prompt, and your store’s customers get instant, accurate answers.\n",
    "\n",
    "👏 **Great job!** Take a moment to celebrate. The skills you used here power most RAG-based chatbots you see everywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eabbd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
